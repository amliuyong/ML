{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gan Summary](Gan_sum.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "hI6euJM4Lbfm",
    "outputId": "f6c9e6f8-f20a-4591-a488-3f56f0d22181"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MmDF5zFxBhTF"
   },
   "outputs": [],
   "source": [
    "# More imports\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU, Dropout, \\\n",
    "  BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XqIB1I8wBkct",
    "outputId": "ffc7dae6-9979-4a3d-9ea6-c0d2070dc4a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Load in the data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# map inputs to (-1, +1) for better training\n",
    "x_train, x_test = x_train / 255.0 * 2 - 1, x_test / 255.0 * 2 - 1\n",
    "print(\"x_train.shape:\", x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GDjyfbCYBxBO"
   },
   "outputs": [],
   "source": [
    "# Flatten the data\n",
    "N, H, W = x_train.shape\n",
    "D = H * W\n",
    "x_train = x_train.reshape(-1, D)\n",
    "x_test = x_test.reshape(-1, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uTbNDWmvCE6V"
   },
   "outputs": [],
   "source": [
    "# Dimensionality of the latent space\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qvm8V2yjB3mB"
   },
   "outputs": [],
   "source": [
    "# Get the generator model\n",
    "def build_generator(latent_dim):\n",
    "  i = Input(shape=(latent_dim,))\n",
    "  x = Dense(256, activation=LeakyReLU(alpha=0.2))(i)\n",
    "  x = BatchNormalization(momentum=0.7)(x)\n",
    "  x = Dense(512, activation=LeakyReLU(alpha=0.2))(x)\n",
    "  x = BatchNormalization(momentum=0.7)(x)\n",
    "  x = Dense(1024, activation=LeakyReLU(alpha=0.2))(x)\n",
    "  x = BatchNormalization(momentum=0.7)(x)\n",
    "  x = Dense(D, activation='tanh')(x)\n",
    "\n",
    "  model = Model(i, x)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X4vBjNK3C2sl"
   },
   "outputs": [],
   "source": [
    "# Get the discriminator model\n",
    "def build_discriminator(img_size):\n",
    "  i = Input(shape=(img_size,))\n",
    "  x = Dense(512, activation=LeakyReLU(alpha=0.2))(i)\n",
    "  x = Dense(256, activation=LeakyReLU(alpha=0.2))(x)\n",
    "  x = Dense(1, activation='sigmoid')(x)\n",
    "  model = Model(i, x)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DDSVYGpUDPaq"
   },
   "outputs": [],
   "source": [
    "# Compile both models in preparation for training\n",
    "\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator(D)\n",
    "discriminator.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(0.0002, 0.5),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# Build and compile the combined model\n",
    "generator = build_generator(latent_dim)\n",
    "\n",
    "# Create an input to represent noise sample from latent space\n",
    "z = Input(shape=(latent_dim,))\n",
    "\n",
    "# Pass noise through generator to get an image\n",
    "img = generator(z)\n",
    "\n",
    "# Make sure only the generator is trained\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The true output is fake, but we label them real!\n",
    "fake_pred = discriminator(img)\n",
    "\n",
    "# Create the combined model object\n",
    "\n",
    "combined_model = Model(input=z, output=fake_pred)\n",
    "\n",
    "# Compile the combined model\n",
    "combined_model.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WLOFTV32EfSr"
   },
   "outputs": [],
   "source": [
    "# Train the GAN\n",
    "\n",
    "\n",
    "# Config\n",
    "batch_size = 32\n",
    "epochs = 30000\n",
    "sample_period = 200 # every `sample_period` steps generate and save some data\n",
    "\n",
    "\n",
    "# Create batch labels to use when calling train_on_batch\n",
    "ones = np.ones(batch_size)\n",
    "zeros = np.zeros(batch_size)\n",
    "\n",
    "# Store the losses\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "\n",
    "# Create a folder to store generated images\n",
    "if not os.path.exists('gan_images'):\n",
    "  os.makedirs('gan_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w7_x3drjGSvU"
   },
   "outputs": [],
   "source": [
    "# A function to generate a grid of random samples from the generator\n",
    "# and save them to a file\n",
    "def sample_images(epoch):\n",
    "  rows, cols = 5, 5\n",
    "  noise = np.random.randn(rows * cols, latent_dim)\n",
    "  imgs = generator.predict(noise)\n",
    "\n",
    "  # Rescale images 0 - 1\n",
    "  imgs = 0.5 * imgs + 0.5\n",
    "\n",
    "  fig, axs = plt.subplots(rows, cols)\n",
    "    \n",
    "  idx = 0\n",
    "  for i in range(rows):\n",
    "    for j in range(cols):\n",
    "      axs[i,j].imshow(imgs[idx].reshape(H, W), cmap='gray')\n",
    "      axs[i,j].axis('off')\n",
    "      idx += 1\n",
    "  fig.savefig(\"gan_images/%d.png\" % epoch)\n",
    "  plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "5ocgiDX_FED9",
    "outputId": "01a776fe-89d8-4580-9748-6207a8dc6f7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/30000, d_loss: 1.01,       d_acc: 0.12, g_loss: 0.70\n",
      "epoch: 101/30000, d_loss: 0.06,       d_acc: 1.00, g_loss: 3.73\n",
      "epoch: 201/30000, d_loss: 0.71,       d_acc: 0.59, g_loss: 1.23\n",
      "epoch: 301/30000, d_loss: 0.73,       d_acc: 0.47, g_loss: 0.59\n",
      "epoch: 401/30000, d_loss: 0.71,       d_acc: 0.48, g_loss: 0.66\n",
      "epoch: 501/30000, d_loss: 0.69,       d_acc: 0.45, g_loss: 0.65\n",
      "epoch: 601/30000, d_loss: 0.69,       d_acc: 0.48, g_loss: 0.65\n",
      "epoch: 701/30000, d_loss: 0.67,       d_acc: 0.59, g_loss: 0.69\n",
      "epoch: 801/30000, d_loss: 0.65,       d_acc: 0.56, g_loss: 0.70\n",
      "epoch: 901/30000, d_loss: 0.67,       d_acc: 0.61, g_loss: 0.67\n",
      "epoch: 1001/30000, d_loss: 0.68,       d_acc: 0.58, g_loss: 0.70\n",
      "epoch: 1101/30000, d_loss: 0.69,       d_acc: 0.50, g_loss: 0.79\n",
      "epoch: 1201/30000, d_loss: 0.65,       d_acc: 0.55, g_loss: 0.73\n",
      "epoch: 1301/30000, d_loss: 0.69,       d_acc: 0.53, g_loss: 0.79\n",
      "epoch: 1401/30000, d_loss: 0.66,       d_acc: 0.61, g_loss: 0.73\n",
      "epoch: 1501/30000, d_loss: 0.66,       d_acc: 0.66, g_loss: 0.77\n",
      "epoch: 1601/30000, d_loss: 0.63,       d_acc: 0.64, g_loss: 0.85\n",
      "epoch: 1701/30000, d_loss: 0.66,       d_acc: 0.58, g_loss: 0.82\n",
      "epoch: 1801/30000, d_loss: 0.65,       d_acc: 0.59, g_loss: 0.84\n",
      "epoch: 1901/30000, d_loss: 0.67,       d_acc: 0.58, g_loss: 0.84\n",
      "epoch: 2001/30000, d_loss: 0.66,       d_acc: 0.64, g_loss: 0.85\n",
      "epoch: 2101/30000, d_loss: 0.64,       d_acc: 0.61, g_loss: 0.87\n",
      "epoch: 2201/30000, d_loss: 0.59,       d_acc: 0.78, g_loss: 0.87\n",
      "epoch: 2301/30000, d_loss: 0.67,       d_acc: 0.56, g_loss: 0.78\n",
      "epoch: 2401/30000, d_loss: 0.64,       d_acc: 0.56, g_loss: 0.86\n",
      "epoch: 2501/30000, d_loss: 0.62,       d_acc: 0.66, g_loss: 0.83\n",
      "epoch: 2601/30000, d_loss: 0.64,       d_acc: 0.62, g_loss: 0.91\n",
      "epoch: 2701/30000, d_loss: 0.69,       d_acc: 0.53, g_loss: 0.82\n",
      "epoch: 2801/30000, d_loss: 0.68,       d_acc: 0.55, g_loss: 0.77\n",
      "epoch: 2901/30000, d_loss: 0.70,       d_acc: 0.50, g_loss: 0.82\n",
      "epoch: 3001/30000, d_loss: 0.65,       d_acc: 0.61, g_loss: 0.78\n",
      "epoch: 3101/30000, d_loss: 0.73,       d_acc: 0.44, g_loss: 0.84\n",
      "epoch: 3201/30000, d_loss: 0.66,       d_acc: 0.67, g_loss: 0.83\n",
      "epoch: 3301/30000, d_loss: 0.65,       d_acc: 0.66, g_loss: 0.82\n",
      "epoch: 3401/30000, d_loss: 0.63,       d_acc: 0.67, g_loss: 0.84\n",
      "epoch: 3501/30000, d_loss: 0.73,       d_acc: 0.42, g_loss: 0.82\n",
      "epoch: 3601/30000, d_loss: 0.68,       d_acc: 0.56, g_loss: 0.82\n",
      "epoch: 3701/30000, d_loss: 0.68,       d_acc: 0.45, g_loss: 0.82\n",
      "epoch: 3801/30000, d_loss: 0.67,       d_acc: 0.58, g_loss: 0.76\n",
      "epoch: 3901/30000, d_loss: 0.70,       d_acc: 0.50, g_loss: 0.79\n",
      "epoch: 4001/30000, d_loss: 0.63,       d_acc: 0.75, g_loss: 0.79\n",
      "epoch: 4101/30000, d_loss: 0.70,       d_acc: 0.50, g_loss: 0.76\n",
      "epoch: 4201/30000, d_loss: 0.68,       d_acc: 0.59, g_loss: 0.80\n",
      "epoch: 4301/30000, d_loss: 0.70,       d_acc: 0.48, g_loss: 0.78\n",
      "epoch: 4401/30000, d_loss: 0.67,       d_acc: 0.53, g_loss: 0.82\n",
      "epoch: 4501/30000, d_loss: 0.64,       d_acc: 0.61, g_loss: 0.78\n",
      "epoch: 4601/30000, d_loss: 0.68,       d_acc: 0.58, g_loss: 0.83\n",
      "epoch: 4701/30000, d_loss: 0.67,       d_acc: 0.53, g_loss: 0.82\n",
      "epoch: 4801/30000, d_loss: 0.66,       d_acc: 0.58, g_loss: 0.85\n",
      "epoch: 4901/30000, d_loss: 0.70,       d_acc: 0.52, g_loss: 0.77\n",
      "epoch: 5001/30000, d_loss: 0.65,       d_acc: 0.56, g_loss: 0.77\n",
      "epoch: 5101/30000, d_loss: 0.62,       d_acc: 0.70, g_loss: 0.76\n",
      "epoch: 5201/30000, d_loss: 0.60,       d_acc: 0.73, g_loss: 0.86\n",
      "epoch: 5301/30000, d_loss: 0.68,       d_acc: 0.56, g_loss: 0.85\n",
      "epoch: 5401/30000, d_loss: 0.66,       d_acc: 0.58, g_loss: 0.80\n",
      "epoch: 5501/30000, d_loss: 0.70,       d_acc: 0.53, g_loss: 0.82\n",
      "epoch: 5601/30000, d_loss: 0.68,       d_acc: 0.56, g_loss: 0.86\n",
      "epoch: 5701/30000, d_loss: 0.69,       d_acc: 0.59, g_loss: 0.85\n",
      "epoch: 5801/30000, d_loss: 0.66,       d_acc: 0.58, g_loss: 0.77\n",
      "epoch: 5901/30000, d_loss: 0.72,       d_acc: 0.47, g_loss: 0.81\n",
      "epoch: 6001/30000, d_loss: 0.64,       d_acc: 0.64, g_loss: 0.91\n",
      "epoch: 6101/30000, d_loss: 0.68,       d_acc: 0.55, g_loss: 0.82\n",
      "epoch: 6201/30000, d_loss: 0.67,       d_acc: 0.55, g_loss: 0.80\n",
      "epoch: 6301/30000, d_loss: 0.65,       d_acc: 0.61, g_loss: 0.78\n",
      "epoch: 6401/30000, d_loss: 0.67,       d_acc: 0.64, g_loss: 0.82\n",
      "epoch: 6501/30000, d_loss: 0.66,       d_acc: 0.56, g_loss: 0.83\n",
      "epoch: 6601/30000, d_loss: 0.68,       d_acc: 0.48, g_loss: 0.81\n",
      "epoch: 6701/30000, d_loss: 0.72,       d_acc: 0.56, g_loss: 0.78\n",
      "epoch: 6801/30000, d_loss: 0.64,       d_acc: 0.66, g_loss: 0.83\n",
      "epoch: 6901/30000, d_loss: 0.68,       d_acc: 0.58, g_loss: 0.77\n",
      "epoch: 7001/30000, d_loss: 0.70,       d_acc: 0.52, g_loss: 0.79\n",
      "epoch: 7101/30000, d_loss: 0.68,       d_acc: 0.59, g_loss: 0.77\n",
      "epoch: 7201/30000, d_loss: 0.68,       d_acc: 0.50, g_loss: 0.82\n",
      "epoch: 7301/30000, d_loss: 0.65,       d_acc: 0.56, g_loss: 0.83\n",
      "epoch: 7401/30000, d_loss: 0.73,       d_acc: 0.45, g_loss: 0.83\n",
      "epoch: 7501/30000, d_loss: 0.65,       d_acc: 0.64, g_loss: 0.78\n",
      "epoch: 7601/30000, d_loss: 0.69,       d_acc: 0.50, g_loss: 0.80\n",
      "epoch: 7701/30000, d_loss: 0.68,       d_acc: 0.58, g_loss: 0.80\n",
      "epoch: 7801/30000, d_loss: 0.74,       d_acc: 0.39, g_loss: 0.79\n",
      "epoch: 7901/30000, d_loss: 0.68,       d_acc: 0.52, g_loss: 0.79\n",
      "epoch: 8001/30000, d_loss: 0.65,       d_acc: 0.61, g_loss: 0.83\n",
      "epoch: 8101/30000, d_loss: 0.66,       d_acc: 0.62, g_loss: 0.79\n",
      "epoch: 8201/30000, d_loss: 0.66,       d_acc: 0.62, g_loss: 0.80\n",
      "epoch: 8301/30000, d_loss: 0.68,       d_acc: 0.56, g_loss: 0.84\n",
      "epoch: 8401/30000, d_loss: 0.73,       d_acc: 0.59, g_loss: 0.83\n",
      "epoch: 8501/30000, d_loss: 0.72,       d_acc: 0.44, g_loss: 0.84\n",
      "epoch: 8601/30000, d_loss: 0.68,       d_acc: 0.56, g_loss: 0.83\n",
      "epoch: 8701/30000, d_loss: 0.64,       d_acc: 0.66, g_loss: 0.74\n",
      "epoch: 8801/30000, d_loss: 0.67,       d_acc: 0.64, g_loss: 0.85\n",
      "epoch: 8901/30000, d_loss: 0.68,       d_acc: 0.48, g_loss: 0.79\n",
      "epoch: 9001/30000, d_loss: 0.75,       d_acc: 0.34, g_loss: 0.77\n",
      "epoch: 9101/30000, d_loss: 0.68,       d_acc: 0.53, g_loss: 0.77\n",
      "epoch: 9201/30000, d_loss: 0.68,       d_acc: 0.58, g_loss: 0.77\n",
      "epoch: 9301/30000, d_loss: 0.67,       d_acc: 0.61, g_loss: 0.75\n",
      "epoch: 9401/30000, d_loss: 0.70,       d_acc: 0.58, g_loss: 0.83\n",
      "epoch: 9501/30000, d_loss: 0.65,       d_acc: 0.62, g_loss: 0.78\n",
      "epoch: 9601/30000, d_loss: 0.64,       d_acc: 0.61, g_loss: 0.84\n",
      "epoch: 9701/30000, d_loss: 0.67,       d_acc: 0.56, g_loss: 0.78\n",
      "epoch: 9801/30000, d_loss: 0.65,       d_acc: 0.55, g_loss: 0.79\n",
      "epoch: 9901/30000, d_loss: 0.70,       d_acc: 0.55, g_loss: 0.82\n",
      "epoch: 10001/30000, d_loss: 0.70,       d_acc: 0.52, g_loss: 0.79\n",
      "epoch: 10101/30000, d_loss: 0.69,       d_acc: 0.56, g_loss: 0.80\n",
      "epoch: 10201/30000, d_loss: 0.66,       d_acc: 0.61, g_loss: 0.84\n",
      "epoch: 10301/30000, d_loss: 0.68,       d_acc: 0.61, g_loss: 0.81\n",
      "epoch: 10401/30000, d_loss: 0.66,       d_acc: 0.67, g_loss: 0.82\n",
      "epoch: 10501/30000, d_loss: 0.67,       d_acc: 0.62, g_loss: 0.82\n",
      "epoch: 10601/30000, d_loss: 0.71,       d_acc: 0.48, g_loss: 0.80\n",
      "epoch: 10701/30000, d_loss: 0.67,       d_acc: 0.62, g_loss: 0.75\n",
      "epoch: 10801/30000, d_loss: 0.71,       d_acc: 0.56, g_loss: 0.84\n",
      "epoch: 10901/30000, d_loss: 0.71,       d_acc: 0.61, g_loss: 0.83\n",
      "epoch: 11001/30000, d_loss: 0.66,       d_acc: 0.62, g_loss: 0.79\n",
      "epoch: 11101/30000, d_loss: 0.60,       d_acc: 0.73, g_loss: 0.78\n",
      "epoch: 11201/30000, d_loss: 0.69,       d_acc: 0.47, g_loss: 0.76\n",
      "epoch: 11301/30000, d_loss: 0.67,       d_acc: 0.56, g_loss: 0.82\n",
      "epoch: 11401/30000, d_loss: 0.71,       d_acc: 0.56, g_loss: 0.78\n",
      "epoch: 11501/30000, d_loss: 0.65,       d_acc: 0.61, g_loss: 0.86\n",
      "epoch: 11601/30000, d_loss: 0.68,       d_acc: 0.59, g_loss: 0.78\n",
      "epoch: 11701/30000, d_loss: 0.68,       d_acc: 0.55, g_loss: 0.77\n",
      "epoch: 11801/30000, d_loss: 0.65,       d_acc: 0.64, g_loss: 0.82\n",
      "epoch: 11901/30000, d_loss: 0.70,       d_acc: 0.53, g_loss: 0.80\n",
      "epoch: 12001/30000, d_loss: 0.70,       d_acc: 0.56, g_loss: 0.84\n",
      "epoch: 12101/30000, d_loss: 0.66,       d_acc: 0.59, g_loss: 0.77\n",
      "epoch: 12201/30000, d_loss: 0.74,       d_acc: 0.41, g_loss: 0.85\n",
      "epoch: 12301/30000, d_loss: 0.67,       d_acc: 0.61, g_loss: 0.84\n",
      "epoch: 12401/30000, d_loss: 0.67,       d_acc: 0.59, g_loss: 0.81\n",
      "epoch: 12501/30000, d_loss: 0.72,       d_acc: 0.50, g_loss: 0.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12601/30000, d_loss: 0.70,       d_acc: 0.58, g_loss: 0.86\n",
      "epoch: 12701/30000, d_loss: 0.67,       d_acc: 0.58, g_loss: 0.79\n",
      "epoch: 12801/30000, d_loss: 0.65,       d_acc: 0.62, g_loss: 0.85\n",
      "epoch: 12901/30000, d_loss: 0.71,       d_acc: 0.53, g_loss: 0.84\n",
      "epoch: 13001/30000, d_loss: 0.67,       d_acc: 0.52, g_loss: 0.83\n",
      "epoch: 13101/30000, d_loss: 0.67,       d_acc: 0.64, g_loss: 0.84\n",
      "epoch: 13201/30000, d_loss: 0.64,       d_acc: 0.59, g_loss: 0.86\n",
      "epoch: 13301/30000, d_loss: 0.69,       d_acc: 0.59, g_loss: 0.82\n",
      "epoch: 13401/30000, d_loss: 0.65,       d_acc: 0.67, g_loss: 0.86\n",
      "epoch: 13501/30000, d_loss: 0.71,       d_acc: 0.58, g_loss: 0.83\n",
      "epoch: 13601/30000, d_loss: 0.67,       d_acc: 0.59, g_loss: 0.83\n",
      "epoch: 13701/30000, d_loss: 0.70,       d_acc: 0.55, g_loss: 0.84\n",
      "epoch: 13801/30000, d_loss: 0.70,       d_acc: 0.47, g_loss: 0.80\n",
      "epoch: 13901/30000, d_loss: 0.70,       d_acc: 0.59, g_loss: 0.78\n",
      "epoch: 14001/30000, d_loss: 0.66,       d_acc: 0.53, g_loss: 0.79\n",
      "epoch: 14101/30000, d_loss: 0.69,       d_acc: 0.53, g_loss: 0.85\n",
      "epoch: 14201/30000, d_loss: 0.64,       d_acc: 0.61, g_loss: 0.81\n",
      "epoch: 14301/30000, d_loss: 0.68,       d_acc: 0.52, g_loss: 0.79\n",
      "epoch: 14401/30000, d_loss: 0.72,       d_acc: 0.48, g_loss: 0.86\n",
      "epoch: 14501/30000, d_loss: 0.73,       d_acc: 0.47, g_loss: 0.83\n",
      "epoch: 14601/30000, d_loss: 0.67,       d_acc: 0.56, g_loss: 0.77\n",
      "epoch: 14701/30000, d_loss: 0.68,       d_acc: 0.61, g_loss: 0.79\n",
      "epoch: 14801/30000, d_loss: 0.68,       d_acc: 0.55, g_loss: 0.82\n",
      "epoch: 14901/30000, d_loss: 0.67,       d_acc: 0.59, g_loss: 0.78\n",
      "epoch: 15001/30000, d_loss: 0.69,       d_acc: 0.53, g_loss: 0.80\n",
      "epoch: 15101/30000, d_loss: 0.66,       d_acc: 0.61, g_loss: 0.76\n",
      "epoch: 15201/30000, d_loss: 0.67,       d_acc: 0.61, g_loss: 0.82\n",
      "epoch: 15301/30000, d_loss: 0.72,       d_acc: 0.45, g_loss: 0.78\n",
      "epoch: 15401/30000, d_loss: 0.67,       d_acc: 0.53, g_loss: 0.83\n",
      "epoch: 15501/30000, d_loss: 0.67,       d_acc: 0.59, g_loss: 0.82\n",
      "epoch: 15601/30000, d_loss: 0.67,       d_acc: 0.53, g_loss: 0.83\n",
      "epoch: 15701/30000, d_loss: 0.72,       d_acc: 0.55, g_loss: 0.88\n",
      "epoch: 15801/30000, d_loss: 0.69,       d_acc: 0.52, g_loss: 0.88\n",
      "epoch: 15901/30000, d_loss: 0.61,       d_acc: 0.64, g_loss: 0.83\n",
      "epoch: 16001/30000, d_loss: 0.70,       d_acc: 0.56, g_loss: 0.82\n",
      "epoch: 16101/30000, d_loss: 0.65,       d_acc: 0.70, g_loss: 0.83\n",
      "epoch: 16201/30000, d_loss: 0.63,       d_acc: 0.67, g_loss: 0.85\n",
      "epoch: 16301/30000, d_loss: 0.66,       d_acc: 0.59, g_loss: 0.76\n",
      "epoch: 16401/30000, d_loss: 0.72,       d_acc: 0.56, g_loss: 0.80\n",
      "epoch: 16501/30000, d_loss: 0.62,       d_acc: 0.69, g_loss: 0.83\n",
      "epoch: 16601/30000, d_loss: 0.72,       d_acc: 0.52, g_loss: 0.84\n",
      "epoch: 16701/30000, d_loss: 0.67,       d_acc: 0.61, g_loss: 0.86\n",
      "epoch: 16801/30000, d_loss: 0.66,       d_acc: 0.53, g_loss: 0.80\n",
      "epoch: 16901/30000, d_loss: 0.67,       d_acc: 0.56, g_loss: 0.75\n",
      "epoch: 17001/30000, d_loss: 0.74,       d_acc: 0.42, g_loss: 0.83\n",
      "epoch: 17101/30000, d_loss: 0.68,       d_acc: 0.59, g_loss: 0.92\n",
      "epoch: 17201/30000, d_loss: 0.65,       d_acc: 0.62, g_loss: 0.86\n",
      "epoch: 17301/30000, d_loss: 0.71,       d_acc: 0.50, g_loss: 0.85\n",
      "epoch: 17401/30000, d_loss: 0.70,       d_acc: 0.53, g_loss: 0.80\n",
      "epoch: 17501/30000, d_loss: 0.67,       d_acc: 0.59, g_loss: 0.75\n",
      "epoch: 17601/30000, d_loss: 0.67,       d_acc: 0.55, g_loss: 0.83\n",
      "epoch: 17701/30000, d_loss: 0.66,       d_acc: 0.62, g_loss: 0.86\n",
      "epoch: 17801/30000, d_loss: 0.76,       d_acc: 0.41, g_loss: 0.79\n",
      "epoch: 17901/30000, d_loss: 0.71,       d_acc: 0.42, g_loss: 0.79\n",
      "epoch: 18001/30000, d_loss: 0.66,       d_acc: 0.64, g_loss: 0.88\n",
      "epoch: 18101/30000, d_loss: 0.77,       d_acc: 0.48, g_loss: 0.78\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-38ad7ab0acbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;31m# Train the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;31m# both loss and accuracy are returned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0md_loss_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_acc_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m   \u001b[0md_loss_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_acc_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md_loss_real\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[1;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[1;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[1;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[1;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    312\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    266\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backwards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaled_total_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m           \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_total_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m           if isinstance(model.optimizer,\n\u001b[1;32m    270\u001b[0m                         loss_scale_optimizer.LossScaleOptimizer):\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_NegGrad\u001b[0;34m(_, grad)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_NegGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m   \u001b[0;34m\"\"\"Returns -grad.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mneg\u001b[0;34m(x, name)\u001b[0m\n\u001b[1;32m   6819\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[1;32m   6820\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Neg\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6821\u001b[0;31m         name, _ctx._post_execution_callbacks, x)\n\u001b[0m\u001b[1;32m   6822\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6823\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main training loop\n",
    "for epoch in range(epochs):\n",
    "  ###########################\n",
    "  ### Train discriminator ###\n",
    "  ###########################\n",
    "  \n",
    "  # Select a random batch of images\n",
    "  idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "  real_imgs = x_train[idx]\n",
    "  \n",
    "  # Generate fake images\n",
    "  noise = np.random.randn(batch_size, latent_dim)\n",
    "  fake_imgs = generator.predict(noise)\n",
    "  \n",
    "  # Train the discriminator\n",
    "  # both loss and accuracy are returned\n",
    "  d_loss_real, d_acc_real = discriminator.train_on_batch(real_imgs, ones)\n",
    "  d_loss_fake, d_acc_fake = discriminator.train_on_batch(fake_imgs, zeros)\n",
    "  d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
    "  d_acc  = 0.5 * (d_acc_real + d_acc_fake)\n",
    "  \n",
    "  \n",
    "  #######################\n",
    "  ### Train generator ###\n",
    "  #######################\n",
    "  \n",
    "  noise = np.random.randn(batch_size, latent_dim)\n",
    "  g_loss = combined_model.train_on_batch(noise, ones)\n",
    "  \n",
    "  # do it again!\n",
    "  noise = np.random.randn(batch_size, latent_dim)\n",
    "  g_loss = combined_model.train_on_batch(noise, ones)\n",
    "  \n",
    "  # Save the losses\n",
    "  d_losses.append(d_loss)\n",
    "  g_losses.append(g_loss)\n",
    "  \n",
    "  if epoch % 100 == 0:\n",
    "    print(f\"epoch: {epoch+1}/{epochs}, d_loss: {d_loss:.2f}, \\\n",
    "      d_acc: {d_acc:.2f}, g_loss: {g_loss:.2f}\")\n",
    "  \n",
    "  if epoch % sample_period == 0:\n",
    "    sample_images(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "n3amVRVBczjW",
    "outputId": "29e3b0dd-00a3-4709-fa51-1ac23d2ce171"
   },
   "outputs": [],
   "source": [
    "plt.plot(g_losses, label='g_losses')\n",
    "plt.plot(d_losses, label='d_losses')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "HxTY_acbJa0k",
    "outputId": "6870e72e-48e2-4100-ad5e-2893834b6774"
   },
   "outputs": [],
   "source": [
    "!ls gan_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "p0lsxXcQMcVz",
    "outputId": "eba3f3cb-4ac5-473b-ab9d-42b825ada31a"
   },
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "a = imread('gan_images/0.png')\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "rJzsSIlRMdMf",
    "outputId": "ecc54161-dd28-4a68-e93f-4faf2248eb07"
   },
   "outputs": [],
   "source": [
    "a = imread('gan_images/1000.png')\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "wBjVwKorMfDy",
    "outputId": "a41fac5d-26e2-435b-e528-bf73b0635cc7"
   },
   "outputs": [],
   "source": [
    "a = imread('gan_images/5000.png')\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "VFAGCNpEMgKY",
    "outputId": "fb2e6fff-605c-4241-e3a4-daa87c6bc605"
   },
   "outputs": [],
   "source": [
    "a = imread('gan_images/10000.png')\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "w4UT1QY4MhOw",
    "outputId": "aaf9050d-362c-4dcc-db06-fe544422cf54"
   },
   "outputs": [],
   "source": [
    "a = imread('gan_images/20000.png')\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "lSTKOUs7JdPQ",
    "outputId": "3c615e8e-07e3-4a01-9595-c8ffae64d1c3"
   },
   "outputs": [],
   "source": [
    "a = imread('gan_images/29800.png')\n",
    "plt.imshow(a)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TF2.0 GAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
